# Mini‑RAG (FAQ + цитаты)

Небольшой, **практичный** пример Retrieval‑Augmented Generation (RAG), ориентированный на банковский/финансовый FAQ.  
Цель — показать **полный цикл** от загрузки документов до выдачи **ответа с цитатами** и дать базу, которую легко
расширить до FAISS/Chroma + эмбеддинги, rerank, guardrails и строгие контракты вывода.

> Подходит для прототипирования и обучения: за счёт TF‑IDF индекс не требует внешних сервисов и GPU.

---

## Ключевые возможности

- **Ингест** `.md`/`.txt` документов и разбиение на чанки‑пассажи.
- **Индекс TF‑IDF** (легкий и переносимый) — удобно для демонстрации пайплайна.
- **Ретривер**: косинусная близость, top‑k.
- **Ответы в двух режимах**:
  1) **Без LLM** — просто выбираем лучшие пассажи и показываем их как “ответ + цитаты”.
  2) **С LLM** — короткий ответ на основе лучших пассажей через OpenAI/GigaChat.
- **Цитирование источников** (название документа и псевдо‑URL).
- **Streamlit‑демо**: интерфейс с настройкой Top‑k и переключателем «Использовать LLM».

---

## Когда это полезно

- Быстро показать **ценность RAG** на локальном наборе FAQ/документации.
- Подготовить **демо для бизнеса**: ответы “по документации” с обязательными ссылками.
- Разобраться в **архитектуре RAG** без эмбеддингов и баз в первом приближении.
- Сделать **минимальный PoC** перед переходом к промышленным компонентам (FAISS/Chroma, rerank, кеши, бэкенд).

---

## Архитектура пайплайна

```
Ингест (samples/faq/*.md)
   ↓  split_into_chunks (400, overlap=60)
Индекс TF‑IDF (TfidfVectorizer, ngram=(1,2))
   ↓
Запрос пользователя (q)
   ↓  search(q) → top‑k пассажей (score, title, url, passage)
Формирование ответа:
   ├─ (A) Без LLM: показываем лучший пассаж + список цитат
   └─ (B) С LLM: prompt с топ‑k пассажами → короткий ответ с [ссылками]
```

> В промышленной версии блок “Индекс” часто заменяют на **эмбеддинги + FAISS/Chroma** и добавляют **reranker**.

---

## Установка и запуск

### Локально
```bash
git clone <репозиторий>
cd mini-rag-faq
pip install -r requirements.txt
streamlit run demo_streamlit.py
```

Откроется UI, где можно задать вопрос и посмотреть найденные пассажи/ответ.

### Streamlit Cloud
- Deploy → GitHub Repo → `demo_streamlit.py`
- Python packages → `requirements.txt`
- При необходимости добавьте **Secrets** (см. ниже), иначе используйте режим **без LLM**.

---

## Конфигурация LLM‑провайдеров (по желанию)

Один из вариантов (через ENV или Streamlit *Secrets*).

### OpenAI‑совместимый API
```
PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
```

### GigaChat
```
PROVIDER=gigachat
GIGACHAT_AUTH=<Authorization Key: base64(client_id:client_secret)>
GIGACHAT_SCOPE=GIGACHAT_API_PERS
GIGACHAT_AUTH_URL=https://ngw.devices.sberbank.ru:9443/api/v2/oauth
GIGACHAT_API_URL=https://gigachat.devices.sberbank.ru/api/v1
# Для облака часто используем:
GIGACHAT_VERIFY=false
GIGACHAT_MODEL=GigaChat
```

> В демо провайдер выбирается автоматически на основе `PROVIDER`. Если переменные не заданы — просто выключите переключатель «Использовать LLM».

---

## Структура проекта

```
mini-rag-faq/
├── demo_streamlit.py            # Streamlit-демо (подхватывает пакет из ./src)
├── requirements.txt
├── README.md
├── src/ragmini/
│   ├── __init__.py
│   ├── ingest.py                # ingest, разбиение, tf-idf индекс
│   ├── search.py                # косинус и top-k отбор
│   └── providers.py             # чат-провайдеры OpenAI/GigaChat
└── samples/faq/*.md             # образцы документов FAQ
```

### Основные модули
- `ingest.py` — функции чтения `.md`, разбиения на чанки и построения TF‑IDF индекса.
- `search.py` — поиск: запрос → вектор → косинусная близость → top‑k пассажей.
- `providers.py` — тонкая обёртка над **OpenAI / GigaChat**, одна сигнатура `chat(messages, ...)`.

---

## Как подготовить собственные данные

1. Сложите `.md`/`.txt` в `samples/faq/` или задайте свою папку.
2. При необходимости измените параметры разбиения в `split_into_chunks(text, size=400, overlap=60)`:
   - `size` — целевой размер чанка в **символах** (для TF‑IDF подходит 300–500);
   - `overlap` — перекрытие для снижения «разрывов» на границах.
3. При большом количестве документов можно:
   - нормализовать заголовки, добавить теги/категории;
   - исключить дубли (по хэшу) и служебные блоки (меню/навигация).
4. Перезапустите демо — индекс построится заново при старте.

---

## Внутренняя логика поиска

- **Векторизация**: `TfidfVectorizer(ngram_range=(1,2))` повышает чувствительность к коротким фразам.
- **Сходство**: косинусная близость между вектором запроса и матрицей документов.
- **Ранжирование**: сортировка по убыванию скора, далее top‑k.
- **Метаданные**: для каждого чанка хранится `title`, `url` (псевдо‑URL), `doc_id`.

Параметры, влияющие на качество/скорость:
- `ngram_range` (bigram лучше ловит устойчивые выражения);
- размер чанка и overlap;
- значение `k` (top‑k пассажей, по умолчанию 5).

---

## Сборка ответа

### Режим A — без LLM
- Берём **лучший пассаж** как краткий ответ.
- Цитаты формируем из первых `k` пассажей: показываем их **источник** (название, псевдо‑URL).
- Подходит для максимально дешёвого и предсказуемого демо.

### Режим B — с LLM
- Формируем prompt из **топ‑k пассажей** (каждый с `[номер]` и источником).
- Пишем короткий ответ “по контексту”; если информации нет — честно сообщаем об этом.
- В ответе **просим указывать [ссылки]** в финале фразы.

> Для продуктивной системы рекомендуется добавить **строгий JSON‑контракт** ответа и валидировать/ремонтировать его (см. соседний проект *Strict JSON + Repair*).

---

## Примеры кода (использование как библиотеки)

```python
from ragmini import read_docs, build_corpus, build_tfidf_index, search

docs = read_docs("samples/faq")
corpus, meta = build_corpus(docs)
vect, mat = build_tfidf_index(corpus)

hits = search("Как перевыпустить карту?", vect, mat, corpus, meta, k=5)
for i, h in enumerate(hits, 1):
    print(i, h["title"], h["score"])
```

С LLM‑ответом:
```python
from ragmini import get_chat_provider
provider = get_chat_provider()
context = "\n\n".join(f"[{i+1}] {h['passage']} (src: {h['url']})" for i,h in enumerate(hits))
messages = [
  {"role":"system","content":"Ты ассистент банка. Отвечай по делу и ссылайся на [номер]."},
  {"role":"user","content": f"Вопрос: как перевыпустить карту?\n\nКонтекст:\n{context}"}
]
answer = provider.chat(messages, temperature=0.0, max_tokens=400)
print(answer)
```

---

## Как перейти к “взрослому” RAG

- **Эмбеддинги + векторная БД**: OpenAI/GigaChat/SBERT эмбеддинги, FAISS/Chroma.
- **Reranker**: bge‑reranker / cross‑encoder (значимо повышает качество top‑k).
- **Кэш/персистентность**: сохранять индекс, не пересчитывать при каждом старте.
- **Structured Output**: строгий JSON контракт (например, `answer`, `citations`, `confidence`)
  + валидация и ремонт через *Strict JSON + Repair*.
- **Guardrails**: фильтры PII, политика ошибок, анти‑prompt‑injection.
- **Оценка**: релевантность/faithfulness (ручной чек‑лист и автоматические метрики).

---

## Известные ограничения

- TF‑IDF чувствителен к формулировкам; семантический поиск на эмбеддингах будет лучше.
- Нет reranker‑а, кеша индекса, асинхронной загрузки документов — по минимуму для понятности.
- Чанк‑размер задан в **символах**, не в токенах (для TF‑IDF это уместно).

---

## Траблшутинг

- **`ModuleNotFoundError: No module named 'ragmini'`**  
  В `demo_streamlit.py` добавлен хук для `sys.path` (папка `src`). Если вы перенесли файл — убедитесь, что он сохранился.
- **Секреты LLM не заданы** — просто выключите переключатель «Использовать LLM».
- **GigaChat OAuth/SSL** — в облаке используйте `GIGACHAT_VERIFY=false` или укажите свой CA‑bundle; проверьте `GIGACHAT_AUTH` и `GIGACHAT_SCOPE`.

---

## Лицензия

MIT — свободно для коммерческого использования/форков/модификаций.
